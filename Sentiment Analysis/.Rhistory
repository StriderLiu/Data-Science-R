prediction_boundary <- function(p,upper,lower){
for(i in c(1:length(p))){
if(p[i] > upper){
p[i] = upper
}
if(p[i] < lower){
p[i] = lower
}
}
return(p)
}
pred.nn.values <- prediction_boundary(pred.nn.values, 8, 1)
table(test$Response, pred.nn.values)
mae <- function(y, pred) {
mean(abs(y - pred))
}
mae(pred.nn.values, test$Response)
plot.nnet(nn)
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(nn)
# First, data preprocessing for training data set
df <- read.csv("/Users/vincentliu/Documents/R_Workspace/DataScience/assign6/train.csv")
# 1.Fill in the missing data with mean
# find all those columns with missing data
df.hasNA <- df[, colSums(is.na(df)) > 0] # 13
#count the ratio of NAs in each columns
rows.num <- nrow(df)
cnt <- sapply(df.hasNA, function(x) sum(is.na(x)))
cnt <- cnt / rows.num
cnt
# Thus, omitt those columns whoes NA rate is larger than 50%
# And we get Employment_Info_1, Employment_Info_4, Employment_Info_6,
# Insurance_History_5, Family_Hist_2, Family_Hist_4, Medical_History_1
# Just IGNORE Family_Hist_3, Family_Hist_5, Medical_History_10,
# Medical_History_15, Medical_History_24, Medical_History_32
# We can fill the missing data in these columns,
# but with whatever value we fill in there will be too many same values in a column,
# which makes the variance of the column very low
# So fill in or not, these columns can't be very useful to us
##Missing values insertion
#Employment_Info_1
m <- mean(df$Employment_Info_1[!is.na(df$Employment_Info_1)])
df$Employment_Info_1[is.na(df$Employment_Info_1)] = m
#Employment_Info_4
m <- mean(df$Employment_Info_4[!is.na(df$Employment_Info_4)])
df$Employment_Info_4[is.na(df$Employment_Info_4)] = m
#Employment_Info_6
m <- mean(df$Employment_Info_6[!is.na(df$Employment_Info_6)])
df$Employment_Info_6[is.na(df$Employment_Info_6)] = m
#Insurance_History_5
m <- mean(df$Insurance_History_5[!is.na(df$Insurance_History_5)])
df$Insurance_History_5[is.na(df$Insurance_History_5)] = m
#Family_Hist_2
m <- mean(df$Family_Hist_2[!is.na(df$Family_Hist_2)])
df$Family_Hist_2[is.na(df$Family_Hist_2)] = m
#Family_Hist_4
m <- mean(df$Family_Hist_4[!is.na(df$Family_Hist_4)])
df$Family_Hist_4[is.na(df$Family_Hist_4)] = m
#Medical_History_1
m <- mean(df$Medical_History_1[!is.na(df$Medical_History_1)])
df$Medical_History_1[is.na(df$Medical_History_1)] = m
library("plyr")
# 2.Transform categorical data into continuous format
# "1-to-N" method
splitCol <- function(df, prefix, seq) {
for(surfix in seq) { # for each column
col.name <- paste0(prefix, surfix)
col <- df[, col.name]
ori <- unique(col)
newNames <- c()
if (length(ori) > 2) {
# for each possible value in this column
for (i in 1:length(ori)) {
val <- strrep('0', length(ori))
substr(val, i, i) <- '1'
col[col == ori[i]] <- val
newNames <- c(newNames, paste0(col.name, "_", as.character(i)))
}
# split column and append the new ones to data frame
list = strsplit(as.character(col), "")
df.add <- ldply(list)
colnames(df.add) <- newNames
for (name in newNames) {
df[, name] <- as.numeric(df.add[, name])
}
df[, col.name] <- NULL
}
else {
for (i in 1:length(ori)) {
col[col == ori[i]] <- (i - 1)
}
df[, col.name] <- col
}
}
df # return the data frame
}
df <- splitCol(df, "Product_Info_", c(1, 5, 6, 7))
df <- splitCol(df, "Employment_Info_", c(3, 5))
df <- splitCol(df, "InsuredInfo_", c(1, 2, 4, 5, 6, 7))
df <- splitCol(df, "Insurance_History_", c(1, 2, 3, 4, 7, 8, 9))
df <- splitCol(df, "Family_Hist_", c(1))
df <- splitCol(df, "Medical_History_", c(3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41))
# 3.Transform Product_Info_2 to numeric data format
vals <- unique(df$Product_Info_2)
i = 1
for (s in vals) {
df <- as.data.frame(lapply(df,function(x) if(is.character(x)|is.factor(x)) gsub(s,i,x) else x))
i <- i + 1
}
df$Product_Info_2 <- as.numeric(levels(df$Product_Info_2))[df$Product_Info_2]
# 4.Omitt dummy columns and those who has too much missing data
df <- subset(df, select = -c(Family_Hist_3, Family_Hist_5, Medical_History_10,
Medical_History_15, Medical_History_24, Medical_History_32,
Medical_Keyword_1, Medical_Keyword_2, Medical_Keyword_3,
Medical_Keyword_4, Medical_Keyword_5, Medical_Keyword_6,
Medical_Keyword_7, Medical_Keyword_8, Medical_Keyword_9,
Medical_Keyword_10, Medical_Keyword_11, Medical_Keyword_12,
Medical_Keyword_13, Medical_Keyword_14, Medical_Keyword_15,
Medical_Keyword_16, Medical_Keyword_17, Medical_Keyword_18,
Medical_Keyword_19, Medical_Keyword_20, Medical_Keyword_21,
Medical_Keyword_22, Medical_Keyword_23, Medical_Keyword_24,
Medical_Keyword_25, Medical_Keyword_26, Medical_Keyword_27,
Medical_Keyword_28, Medical_Keyword_29, Medical_Keyword_30,
Medical_Keyword_31, Medical_Keyword_32, Medical_Keyword_33,
Medical_Keyword_34, Medical_Keyword_35, Medical_Keyword_36,
Medical_Keyword_37, Medical_Keyword_38, Medical_Keyword_39,
Medical_Keyword_40, Medical_Keyword_41, Medical_Keyword_42,
Medical_Keyword_43, Medical_Keyword_44, Medical_Keyword_45,
Medical_Keyword_46, Medical_Keyword_47, Medical_Keyword_48))
# should be False which means no NA exists
unique(apply(df, 2, function(x) any(is.na(x))))
# 5.Run Logistic Regression and perform dimension reduction
loglm <- glm(Response ~ ., data = df)
summary(loglm)
# dimention reduction
# select (Product_Info_1, Product_Info_2, Product_Info_3, Product_Info_4, Product_Info_5, Product_Info_6, Ins_Age, Ht, Wt, BMI,
# Employment_Info_3, Employment_Info_6, InsuredInfo_2, InsuredInfo_5, InsuredInfo_6, InsuredInfo_7, Insurance_History_1,
# Insurance_History_5, Family_Hist_2, Family_Hist_4, Medical_History_1, Medical_History_2, Medical_History_4, Medical_History_22,
# Medical_History_33, Medical_History_38, Insurance_History_4_1, Insurance_History_4_2, Insurance_History_4_3,
# Insurance_History_7_1,Insurance_History_7_2, Insurance_History_7_3, Medical_History_7_1, Medical_History_7_2, Medical_History_7_3,
# Medical_History_11_1, Medical_History_11_2, Medical_History_11_3, Medical_History_14_1, Medical_History_26_1, Medical_History_26_2, Medical_History_26_3
# Medical_History_35_1, Medical_History_35_2, Medical_History_35_3, Medical_History_39_1, Medical_History_39_2, Medical_History_39_3)
df <- subset(df, select = c(Product_Info_1, Product_Info_2, Product_Info_3, Product_Info_4, Product_Info_5, Product_Info_6, Ins_Age, Ht, Wt, BMI,
Employment_Info_3, Employment_Info_6, InsuredInfo_2, InsuredInfo_5, InsuredInfo_6, InsuredInfo_7, Insurance_History_1,
Insurance_History_5, Family_Hist_2, Family_Hist_4, Medical_History_1, Medical_History_2, Medical_History_4, Medical_History_22,
Medical_History_33, Medical_History_38, Insurance_History_4_1, Insurance_History_4_2, Insurance_History_4_3,
Insurance_History_7_1,Insurance_History_7_2, Insurance_History_7_3, Medical_History_7_1, Medical_History_7_2, Medical_History_7_3,
Medical_History_11_1, Medical_History_11_2, Medical_History_11_3, Medical_History_14_1, Medical_History_14_2, Medical_History_14_3,
Medical_History_26_1, Medical_History_26_2, Medical_History_26_3, Medical_History_35_1, Medical_History_35_2, Medical_History_35_3,
Medical_History_39_1, Medical_History_39_2, Medical_History_39_3, Response))
# write.csv(df, "cleanedDF.csv")
# df <- df[1:20000, ]
# 6.Split the data set into training set and cross validation set
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)*10/100))
test <- df[testindex,]
train <- df[-testindex,]
# 7.Apply NN:
feats <- names(subset(train, select = -c(Response)))
# Concatenate strings
f <- paste(feats, collapse = ' + ')
f <- paste('Response ~', f)
# Convert to formula
f <- as.formula(f)
# Train model with f
# library(neuralnet)
# nn <- neuralnet(f, train, hidden = 10, stepmax = 1e+06, linear.output = TRUE)
# Compute Predictions off Test Set
# pred.nn.values <- compute(nn, test[, 1:50])
# Check out net.result
# print(head(pred.nn.values$net.result))
# pred.nn.values$net.result <- sapply(pred.nn.values$net.result, round, digits = 0)
# table(test$Response, pred.nn.values$net.result)
# accuracy <- sum(!is.na(match(test$Response, pred.nn.values$net.result))) / length(test$Response)
library(nnet)
nn <- nnet(f, data = train, maxit = 800, size = 20, linout = TRUE)
pred.nn.values <- predict(nn, test[, 1:50])
pred.nn.values <- sapply(pred.nn.values, round, digits = 0)
prediction_boundary <- function(p,upper,lower){
for(i in c(1:length(p))){
if(p[i] > upper){
p[i] = upper
}
if(p[i] < lower){
p[i] = lower
}
}
return(p)
}
pred.nn.values <- prediction_boundary(pred.nn.values, 8, 1)
table(test$Response, pred.nn.values)
mae <- function(y, pred) {
mean(abs(y - pred))
}
mae(pred.nn.values, test$Response)
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(nn)
pokemons <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/300k.csv")
head(pokemons$latitude)
pokemons$latitude[1:10]
rm(list = ls())
pokemons <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/300k.csv")
str(pokemons)
pokemons <- pokemons[pokemons$continent == "America"]
pokemons <- pokemons[, pokemons$continent == "America"]
pokemons <- pokemons[pokemons$continent == "America"]
pokemons <- pokemons[, pokemons$continent == "America"]
unique(pokemons$city)
coordinates <- pokemons[, c(2, 3)]
head(coordinates)
length(unique(coordinates))
unique(coordinates)
str(pokemons)
cities <- unique(pokemons$city)
cities
length(unique(pokemons$city))
names(pokemons$city)[22]
names(pokemons$city)
colnames(pokemons)[22]
usPokes <- subset(pokemons, select = c("New_York", "Los_Angeles", "Chicago", "Phoenix",
"Denver", "Indianapolis", "Detroit", "Boise",
"Louisville", "Monroviad"))
usPokes <- pokemons[pokemons$city %in% c("New_York", "Los_Angeles", "Chicago",
"Phoenix", "Denver", "Indianapolis",
"Detroit", "Boise", "Louisville",
"Monroviad"), ]
View(usPokes)
usPokes_1 <- usPokes[1:60000, ]
usPokes_2 <- usPokes[60001:end, ]
usPokes_2 <- usPokes[60001:last, ]
usPokes_1 <- head(usPokes, 60000)
usPokes_2 <- tail(usPokes, length(usPokes) - 60000)
length(usPokes)
usPokes_2 <- tail(usPokes, nrow(usPokes) - 60000)
data <- read.csv("/Users/vincentliu/Documents/R_Workspace/DataScience/onclass/AmazonReview.csv")
str(data)
rm(list = ls())
zip.2500 <- read.table("/Users/vincentliu/Desktop/zips1.txt", header = FALSE)
zip.500 < read.table("/Users/vincentliu/Documents/Scala_Workspace/PikaPika/ZipCodes.txt", header = FALSE)
zip.500 <- read.table("/Users/vincentliu/Documents/Scala_Workspace/PikaPika/ZipCodes.txt", header = FALSE)
zip.500
class(zip.500)
class(zip.500[1])
length(which(zip.500 == "00000"))
length(which(zip.500 == 0))
length(which(zip.500 == 0))
length(which(zip.2500 == 0))
length(which(zip.2500 == "2500"))
length(which(zip.2500 == "00000"))
length(which(zip.2500 == 0))
all(zip.500 == zip.2500[1:500])
all(zip.500[V1] == zip.2500[1:500, V1])
zip.500 == zip.2500
all(zip.500 == zip.2500[1:500, ])
zip.2500 <- read.table("/Users/vincentliu/Desktop/zips1.txt", header = FALSE)
zip.500 <- read.table("/Users/vincentliu/Documents/Scala_Workspace/PikaPika/ZipCodes.txt", header = FALSE)
length(which(zip.500 == 0))
length(which(zip.2500 == 0))
rm(list = ls())
zip.2500 <- read.table("/Users/vincentliu/Desktop/zips1.txt", header = FALSE)
zip.1000 <- read.table("/Users/vincentliu/Documents/Scala_Workspace/PikaPika/ZipCodes.txt", header = FALSE)
length(which(zip.1000 == 0))
length(which(zip.2500 == 0))
all(zip.1000 == zip.2500[1:1000, ])
zip.1000 == zip.2500[1:1000, ]
rm(list = ls())
zip.2500 <- read.tarmble("/Users/vincentliu/Desktop/zips1.txt", header = FALSE)
zip.1500 <- read.table("/Users/vincentliu/Documents/Scala_Workspace/PikaPika/ZipCodes.txt", header = FALSE)
length(which(zip.1500 == 0))
length(which(zip.2500 == 0))
all(zip.1000 == zip.2500[1:1500, ])
zip.2500 <- read.tarmble("/Users/vincentliu/Desktop/zips1.txt", header = FALSE)
zip.1500 <- read.table("/Users/vincentliu/Documents/Scala_Workspace/PikaPika/ZipCodes.txt", header = FALSE)
length(which(zip.1500 == 0))
length(which(zip.2500 == 0))
all(zip.1500 == zip.2500[1:1500, ])
zip.2500 <- read.table("/Users/vincentliu/Desktop/zips1.txt", header = FALSE)
zip.1500 <- read.table("/Users/vincentliu/Documents/Scala_Workspace/PikaPika/ZipCodes.txt", header = FALSE)
length(which(zip.1500 == 0))
length(which(zip.2500 == 0))
all(zip.1500 == zip.2500[1:1500, ])
zip.1500 == zip.2500[1:1500, ]
pokemons <- read.csv("/Users/vincentliu/Documents/Scala_Workspace/PikaPika/DataFiles/300k.csv")
cities <- unique(pokemons$city)
cities
usPokes <- pokemons[pokemons$city %in% c("New_York", "Los_Angeles", "Chicago",
"Phoenix", "Denver", "Indianapolis",
"Detroit", "Boise", "Louisville",
"Monroviad"), ]
nrow(usPokes)
rm(list = ls())
df <- read.csv("/Users/vincentliu/Documents/Scala_Workspace/PikaPika/DataFiles/300k.csv")
str(df)
df.hasNA <- df[, colSums(is.na(df)) > 0]
rows.num <- nrow(df)
cnt <- sapply(df.hasNA, function(x) sum(is.na(x)))
cnt <- cnt / rows.num
cnt
View(df)
View(df)
apply(df, 2, function(x) any(is.na(x)))
unique(apply(df, 2, function(x) any(is.na(x))))
View(df)
colnames(df)
head(df$class)
df <- df[, 2:end]
df <- df[, 2:last]
df <- df[, 2:-1]
df <- df[, -1]
rnames(df)
rname(df)
rownames(df)
colnames(df)
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)*20/100))
test <- df[testindex,]
train <- df[-testindex,]
# Run logistic regression
loglm <- glm(class ~ ., data = df)
View(df)
df$appearedLocalTime
View(df)
rm(list = ls())
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/300k.csv")
unique(df$appearedYear)
unique(df$appearedMonth)
unique(df$appearedDay)
View(df)
preferedDF <- subset(df, -c(pokemonId, appearedLocalTime, X_id, cellId_90m, cellId_180m,
cellId_370m, cellId_730m, cellId_1460m, cellId_2920m, cellId_5850m, appearedDayOfWeek,
appearedMonth, appearedYear, weatherIcon))
preferedDF <- subset(df, select = -c(pokemonId, appearedLocalTime, X_id, cellId_90m, cellId_180m,
cellId_370m, cellId_730m, cellId_1460m, cellId_2920m, cellId_5850m, appearedDayOfWeek,
appearedMonth, appearedYear, weatherIcon))
View(preferedDF)
colnames(preferedDF)
unique(preferedDF$appearedTimeOfDay)
View(df)
rm(list = ls())
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/poke.csv")
ncol(df)
View(df)
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/poke_us.csv")
nrow(df)
nrow(df)
ncol(df)
View(df)
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/poke_us.csv")
ncol(df)
View(df)
str(df)
"?" %in% df
0 %in% df
apply(df, 2, function(x) any(is.na(x)))
unique(apply(df, 2, function(x) any(is.na(x))))
grep("?", df)
class(df$pokestopDistanceKm)
class(df$longitude)
View(df)
class(df$pokestopDistanceKm[1])
df$pokestopDistanceKm[1]
df$pokestopDistanceKm[4]
df$pokestopDistanceKm(1)
class(df$pokestopDistanceKm[[1]])
df$pokestopDistanceKm[[1]]
df$pokestopDistanceKm[[4]]
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/poke_us.csv")
str(df)
# Split the data set into training set and test set
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)*30/100))
test <- df[testindex,]
train <- df[-testindex,]
# Run logistic regression
View(df)
df[which(df$gymIn5000m == '?'), ]
length(which(df$gymIn5000m == "?"))
apply(df, 2, function(x) "?" in x)
apply(df, 2, function(x) "?" %in% x)
res <- apply(df, 2, function(x) "?" %in% x)
unique(res)
which(res == TRUE)
length(df[df$pokestopDistanceKm == "?", ])
unique(df$pokestopDistanceKm)
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/300k.csv")
res <- apply(df, 2, function(x) "?" %in% x)
which(res == TRUE)
length(df[df$pokestopDistanceKm == "?", ])
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/poke_us.csv")
res <- apply(df, 2, function(x) "?" %in% x)
which(res == TRUE)
length(df[df$pokestopDistanceKm == '?', ])
which(df[df$pokestopDistanceKm == '?', ])
length(which(df$pokestopDistanceKm == '?'))
tempVec = df$pokestopDistanceKm
# runif(n, min, max)
minimum <- min(tempVec)
tempVec = as.Vector(df$pokestopDistanceKm)
tempVec = as.vector(df$pokestopDistanceKm)
minimum <- min(tempVec)
maximum <- max(tempVec)
tempVec <- as.vector(df$pokestopDistanceKm)
tempVecWithoutQM <- temVec[temVec != '?']
tempVecWithoutQM <- tempVec[tempVec != '?']
# runif(n, min, max)
minimum <- min(tempVec)
maximum <- max(tempVec)
minimum <- min(tempVecWithoutQM)
maximum <- max(tempVecWithoutQM)
ncol(df)
colnames(df)
View(df)
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/poke_us.csv")
# Detect "?" in dataframe
# res <- apply(df, 2, function(x) "?" %in% x)
# which(res == TRUE)
length(which(df$pokestopDistanceKm == '?'))
# Delete illegal enties
poke <- df[df$pokestopDistanceKm != '?', ]
# Split the data set into training set and test set
index <- 1:nrow(poke)
testindex <- sample(index, trunc(length(index)*30/100))
test <- poke[testindex,]
train <- poke[-testindex,]
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/poke.csv")
# Detect "?" in dataframe
res <- apply(df, 2, function(x) "?" %in% x)
which(res == TRUE)
length(which(df$pokestopDistanceKm == '?'))
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/poke_rarity.csv")
ncol(df)
rm(list = ls())
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/poke_type.csv")
nrow(df)
ncol(df)
View(df)
unique(df$type)
df$rarity[1:200]
rm(list = ls())
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/poke_type.csv")
View(df)
df$type[1:200]
ncol(df)
df$rarity[1:100]
rm(list = ls())
df <- read.csv("/Users/vincentliu/Desktop/Courses_2016Fall/CSYE7200_Scala/Final Project/poke_type.csv")
df$rarity[1:100]
df$type[1:100]
ncol(df)
length(unique(df$type))
View(df)
rm(list = ls())
install.packages("rvest")
library(rvest)
library(plyr)
library(stringr)
library(SnowballC)
setwd('/Users/vincentliu/Desktop/')
pos <- scan(file.path('positive-words.txt'), what='character', comment.char=';')
neg <- scan(file.path('negative-words.txt'), what='character', comment.char=';')
setwd('/Users/vincentliu/Desktop/Courses_2016Fall/INFO7390_DataScience/Assignments//Users/vincentliu/Desktop/Courses_2016Fall/INFO7390_DataScience/Assignments/Assignments 8')
library(plyr)
library(stringr)
library(SnowballC)
setwd('/Users/vincentliu/Desktop/Courses_2016Fall/INFO7390_DataScience/Assignments')
pos <- scan(file.path('positive-words.txt'), what='character', comment.char=';')
pos <- scan(file.path('positive-words.txt'), what='character', comment.char=';')
setwd('/Users/vincentliu/Documents/R_Workspace/DataScience/assign8')
pos <- scan(file.path('positive-words.txt'), what='character', comment.char=';')
pos <- scan(file.path('goodWords.txt'), what='character', comment.char=';')
neg <- scan(file.path('badWords.txt'), what='character', comment.char=';')
Data <- read.csv('northeastern_class.csv',header = TRUE)
# Extract reviews and stem dictionaries
reviews <- lapply(Data$Body, as.character)
pos <- wordStem(pos)
neg <- wordStem(neg)
pos[1:200]
len <- length(reviews)
pos_score <- matrix(0, len, 1)
neg_score < matrix(0, len, 1)
len <- length(reviews)
pos_score <- matrix(0, len, 1)
neg_score < matrix(0, len, 1)
neg_score <- matrix(0, len, 1)
# Compare each stemmed review with the stemmed positive/negative dictionary
for (i in c(1 : len)){
reviews[i] <- gsub('[[:punct:]]', '', reviews[i])
reviews[i] <- gsub('[[:cntrl:]]', '', reviews[i])
reviews[i] <- gsub('\\d+', '', reviews[i])
reviews[i] <- tolower(reviews[i])
word.list <- str_split(reviews[i], '\\s+')
words <- wordStem(unlist(word.list))
pos.matches <- match(words, pos)
neg.matches <- match(words, neg)
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
# Compute the percentage of positive and negative words in each review
pos_score[i] <- sum(pos.matches)/(sum(pos.matches)+sum(neg.matches))
neg_score[i] <- sum(neg.matches)/(sum(pos.matches)+sum(neg.matches))
}
# Calculate average sentiments
mean_neg <- mean(neg_score)
mean_pos <- mean(pos_score)
reviews[1]
library(rvest)
data <- html("http://www.imdb.com/title/tt1490017/")
rm(list = ls())
data <- html("http://www.imdb.com/title/tt1490017/")
data
vignette("selectorgadget")
